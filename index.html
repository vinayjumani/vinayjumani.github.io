<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Vinay R Jumani</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Vinay R Jumani — Deep Learning, AI Hardware, HPC and systems.">
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
</head>
<body>
  <header class="site-header">
    <div class="site-header-inner">
      <div>
        <div class="brand-title">Vinay R Jumani</div>
        <div class="brand-tagline">I like to work on cutting edge research.</div>
      </div>
      <nav class="site-nav">
        <a href="index.html">Home</a>
        <a href="research.html">Research</a>
        <a href="projects.html">Projects</a>
        <a href="experience.html">Experience</a>
        <a href="education.html">Education</a>
        <a href="achievements.html">Achievements</a>
        <a href="extra.html">Extra</a>
        <a href="timeline.html">Timeline</a>
      </nav>
    </div>
  </header>

  <main class="page">
    <!-- HERO -->
    <section class="hero">
      <div>
        <h1>Hi, I’m Vinay.</h1>
        <p>
          I’m an undergraduate at BITS Pilani, Goa working at the intersection of
          deep learning, AI hardware, and high-performance computing. I enjoy understanding systems from first principles:
          how models are built, how they hit the GPU, and how to make that entire path faster and more reliable. And most importantly, I love to explore and play around with lots of stuff in tech. 
        </p>
        <p class="muted">
          <i class="fa-regular fa-envelope"></i>
          <a href="mailto:vinayrjumani@gmail.com">vinayrjumani@gmail.com</a> ·
          <i class="fa-brands fa-github"></i>
          <a href="https://github.com/Vinay12345-neutron" target="_blank">github.com/Vinay12345-neutron</a> ·
          <i class="fa-brands fa-linkedin"></i>
          <a href="https://www.linkedin.com/in/vinay-r-jumani-39489a210" target="_blank">linkedin</a>
        </p>
      </div>

      <div class="avatar-box">
        <!-- Replace with an actual image later -->
        <div class="avatar-circle">
          
          <img src="vinay.jpeg" alt="Vinay R Jumani">
          
          
        </div>
        <div class="muted small">
          B.E. Electronics &amp; Instrumentation<br>BITS Pilani, Goa Campus
        </div>
      </div>
    </section>

    <!-- CURRENT WORK (RESEARCH PREVIEW) -->
    <section class="card">
      <div class="section-title">Current work</div>
      <div class="section-subtitle">A brief snapshot. See <a href="research.html">Research</a> for more detail.</div>

      <div class="item-block">
        <div class="item-header">
          <div class="item-title">EEG–LLM cognitive decoding</div>
          <div class="item-meta">NTU Singapore · Dec 2025 – present</div>
        </div>
        <p class="small">
          Decoding EEG signals into language space using LLMs, focusing on cognitive representations and neural–linguistic
          alignment. Deep learning pipelines for EEG preprocessing, time–frequency transforms, and multimodal embeddings.
        </p>
      </div>

      <div class="item-block">
        <div class="item-header">
          <div class="item-title">Multi-agent LLM inference optimisation</div>
          <div class="item-meta">TCS Research · Nov 2025 – present</div>
        </div>
        <p class="small">
          Optimising multi-agent LLM systems with better routing and agent selection strategies. Profiling inference graphs,
          applying batching, caching and quantisation to bring down p90/p99 latency.
        </p>
      </div>

      <div class="item-block">
        <div class="item-header">
          <div class="item-title">GPU-accelerated HDF5 I/O</div>
          <div class="item-meta">DaSH Lab, BITS Goa · Aug 2025 – present</div>
        </div>
        <p class="small">
          Exploring GPU-Direct Storage, HDF5 extensions and parallel I/O for large-scale ML training. Profiling bandwidth,
          PCIe usage and CPU–GPU transfer bottlenecks.
        </p>
      </div>
    </section>

    <!-- SELECTED PROJECTS PREVIEW -->
    <section class="card">
      <div class="section-title">Selected projects</div>
      <div class="section-subtitle">More details in <a href="projects.html">Projects</a>.</div>

      <div class="item-block">
        <div class="item-header">
          <div class="item-title">FlashAttention-2 Implementation — CUDA & Triton</div>
          <div class="item-meta">
            CUDA · 2025
            
          </div>
        </div>
        <p class="small">
          Implemented the FlashAttention-2 algorithm (Tri Dao et al.) for efficient transformer training, inspired by OpenAI's Fused Attention work. Focused on optimizing attention computation and memory usage
        </p>
      </div>

      <div class="item-block">
        <div class="item-header">
          <div class="item-title">CUDA MatMul Optimization</div>
          <div class="item-meta">
            CUDA · 2025 ·
            <a href="https://github.com/Vinay12345-neutron/DaSH-Lab-Induction-Assignment-2025" target="_blank">code</a>
          </div>
        </div>
        <p class="small">
          Designed & implemented a full CUDA GEMM optimization pipeline, and achieved close to 92% performance of cuBLAS. Also analysed vLLM with Paged Attention paper. 
        </p>
      </div>

      <div class="item-block">
        <div class="item-header">
          <div class="item-title">Transformer from scratch</div>
          <div class="item-meta">
            PyTorch · 2025 ·
            <a href="https://github.com/Vinay12345-neutron/transformer_from_scratch" target="_blank">code</a>
          </div>
        </div>
        <p class="small">
          Full implementation of the Transformer architecture in PyTorch: embeddings, positional encodings, multi-head
          attention, encoder–decoder blocks and training loop. Built mainly to understand the architecture deeply,
          including attention visualisation and ablation.
        </p>
      </div>
    </section>

    <footer class="site-footer">
      <div>© <span id="year"></span> Vinay R Jumani</div>
    </footer>
  </main>

  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
</body>
</html>
